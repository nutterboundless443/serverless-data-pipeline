service: serverless-data-pipeline

provider:
  name: aws
  runtime: python3.8
  stage: dev
  region: us-east-1

functions:
  dataProcessor:
    handler: src/handler.process_data
    events:
      - http:
          path: upload
          method: post

resources:
  Resources:
    # S3 Bucket to store data files
    DataBucket:
      Type: 'AWS::S3::Bucket'
      Properties:
        BucketName: my-serverless-data-pipeline-bucket

    # DynamoDB table to store processed data
    DataTable:
      Type: 'AWS::DynamoDB::Table'
      Properties:
        TableName: DataTable
        AttributeDefinitions:
          # Define the primary key 'id' of type String
          - AttributeName: id
            AttributeType: S
        KeySchema:
          # Specify 'id' as the HASH key
          - AttributeName: id
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1

    # Define IAM role for the Lambda function to interact with S3 and DynamoDB
    LambdaExecutionRole:
      Type: 'AWS::IAM::Role'
      Properties:
        AssumeRolePolicyDocument:
          Version: '2012-10-17'
          Statement:
            - Effect: Allow
              Principal:
                Service: lambda.amazonaws.com
              Action: sts:AssumeRole
        Policies:
          - PolicyName: S3AndDynamoDBAccess
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - s3:PutObject
                    - dynamodb:PutItem
                    - dynamodb:GetItem
                  Resource:
                    - arn:aws:s3:::my-serverless-data-pipeline-bucket/*
                    - arn:aws:dynamodb:${AWS::Region}:${AWS::AccountId}:table/DataTable